{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.get_data import load_benchmark, load_synthetic\n",
    "from src.normalization import get_adj_feats\n",
    "from src.args import get_args\n",
    "from src.models import get_model\n",
    "from src.utils import accuracy, LDA_loss\n",
    "from src.plots import plot_feature\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading citeseer dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenye/Desktop/Dissecting_GNN_final-master/src/get_data.py:95: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish load data\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# all tensor, dense\n",
    "dataset_name = 'citeseer'\n",
    "# dataset_name = input('input dataset name: cora/citeseer/pubmed/...')\n",
    "\n",
    "adj, feats, labels, idx_train, idx_val, idx_test = load_benchmark(dataset_name)\n",
    "# adj, feats, labels, idx_train, idx_val, idx_test = load_synthetic(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get args\n",
    "# model_name = input('choose model: GCN/SGC/GFNN/GFN/AGNN/GIN/...')\n",
    "model_name = 'AGNN'\n",
    "args = get_args(model_opt = model_name, dataset = dataset_name)\n",
    "weights= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, feats = get_adj_feats(adj = adj, feats = feats, model_opt = model_name, degree = args.degree, weights = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_class = (torch.max(labels) + 1).numpy()\n",
    "Y_onehot =  torch.zeros(labels.shape[0], nb_class).scatter_(1, labels.unsqueeze(-1), 1)\n",
    "\n",
    "nb_each_class_train = torch.sum(Y_onehot[idx_train], dim = 0)\n",
    "nb_each_class_inv_train = torch.tensor(np.power(nb_each_class_train.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_train = torch.diag(nb_each_class_inv_train)\n",
    "\n",
    "nb_each_class_val = torch.sum(Y_onehot[idx_val], dim = 0)\n",
    "nb_each_class_inv_val = torch.tensor(np.power(nb_each_class_val.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_val = torch.diag(nb_each_class_inv_val)\n",
    "\n",
    "nb_each_class_test = torch.sum(Y_onehot[idx_test], dim = 0)\n",
    "nb_each_class_inv_test = torch.tensor(np.power(nb_each_class_test.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_test = torch.diag(nb_each_class_inv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "mapping \t torch.Size([6, 6])\n",
      "gc1.weight \t torch.Size([3703, 6])\n",
      "gc1.linear_weight \t torch.Size([7])\n",
      "gc1.bias \t torch.Size([6])\n",
      "optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.001, 'amsgrad': False, 'params': [4891145704, 4891145920, 4891145848, 4891145776]}]\n"
     ]
    }
   ],
   "source": [
    "# get model\n",
    "model = get_model(model_opt = model_name, nfeat = feats.size(1), \\\n",
    "                  nclass = labels.max().item()+1, nhid = args.hidden, \\\n",
    "                  dropout = args.dropout, cuda = args.cuda, \\\n",
    "                  dataset = dataset_name, degree = args.degree)\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    if model_name!='AGNN' and model_name!='GIN':\n",
    "        model.cuda()\n",
    "        feats = feats.cuda()\n",
    "        adj = adj.cuda()\n",
    "        labels = labels.cuda()\n",
    "        idx_train = idx_train.cuda()\n",
    "        idx_val = idx_val.cuda()\n",
    "        idx_test = idx_test.cuda()\n",
    "    \n",
    "    \n",
    "# Print model's state_dict    \n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor,\"\\t\",model.state_dict()[param_tensor].size()) \n",
    "print(\"optimizer's state_dict:\")\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name,\"\\t\",optimizer.state_dict()[var_name])\n",
    "    \n",
    "# # Print parameters\n",
    "# for name,param in model.named_parameters():\n",
    "#     print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output, fp1, fp2 = model(feats, adj)\n",
    "    CE_loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    if model_name == 'AGNN':\n",
    "        LDA_loss_train = LDA_loss(fp1[idx_train], Y_onehot[idx_train], nb_each_class_inv_mat_train, norm_or_not = False)\n",
    "#         loss_train =  CE_loss_train - LDA_loss_train\n",
    "        loss_train =  - LDA_loss_train\n",
    "\n",
    "    else:\n",
    "        loss_train = CE_loss_train\n",
    "    \n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output, fp1, fp2 = model(feats, adj)\n",
    "    \n",
    "    CE_loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "#     loss_val = CE_loss_val\n",
    "    LDA_loss_val = LDA_loss(fp1[idx_val], Y_onehot[idx_val], nb_each_class_inv_mat_val, norm_or_not = True)\n",
    "    loss_val = - LDA_loss_val\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    CE_loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "#     loss_test = CE_loss_test\n",
    "    LDA_loss_test = LDA_loss(fp1[idx_test], Y_onehot[idx_test], nb_each_class_inv_mat_test, norm_or_not = True)\n",
    "    loss_test = - LDA_loss_test\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "#           'loss_test: {:.4f}'.format(loss_test.item()),\n",
    "#           'acc_test: {:.4f}'.format(acc_test.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return epoch+1, loss_train.item(), acc_train.item(), loss_val.item(), \\\n",
    "            acc_val.item(), loss_test.item(), acc_test.item(), time.time() - t, \\\n",
    "            \n",
    "\n",
    "\n",
    "# def test():\n",
    "#     model.eval()\n",
    "#     output, fp1, fp2 = model(feats, adj)\n",
    "#     loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "#     acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "#     print(\"Test set results:\",\n",
    "#           \"loss= {:.4f}\".format(loss_test.item()),\n",
    "#           \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "#     return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: -81.8612 acc_train: 0.1667 loss_val: -72.9693 acc_val: 0.1880 time: 2.6709s\n",
      "Epoch: 0002 loss_train: -82.6054 acc_train: 0.1667 loss_val: -73.8103 acc_val: 0.1880 time: 2.1588s\n",
      "Epoch: 0003 loss_train: -83.3120 acc_train: 0.1667 loss_val: -74.6017 acc_val: 0.1880 time: 1.7595s\n",
      "Epoch: 0004 loss_train: -83.9686 acc_train: 0.1667 loss_val: -75.3373 acc_val: 0.1880 time: 2.3347s\n",
      "Epoch: 0005 loss_train: -84.5673 acc_train: 0.1667 loss_val: -76.0170 acc_val: 0.1880 time: 1.5639s\n",
      "Epoch: 0006 loss_train: -85.1058 acc_train: 0.1667 loss_val: -76.6476 acc_val: 0.1880 time: 1.5209s\n",
      "Epoch: 0007 loss_train: -85.5868 acc_train: 0.1667 loss_val: -77.2406 acc_val: 0.1880 time: 1.7539s\n",
      "Epoch: 0008 loss_train: -86.0176 acc_train: 0.1667 loss_val: -77.8069 acc_val: 0.1880 time: 1.4946s\n",
      "Epoch: 0009 loss_train: -86.4071 acc_train: 0.1667 loss_val: -78.3524 acc_val: 0.1880 time: 1.6059s\n",
      "Epoch: 0010 loss_train: -86.7629 acc_train: 0.1667 loss_val: -78.8772 acc_val: 0.1880 time: 1.6768s\n",
      "Epoch: 0011 loss_train: -87.0898 acc_train: 0.1667 loss_val: -79.3770 acc_val: 0.1880 time: 1.5650s\n",
      "Epoch: 0012 loss_train: -87.3905 acc_train: 0.1667 loss_val: -79.8470 acc_val: 0.1880 time: 1.6009s\n",
      "Epoch: 0013 loss_train: -87.6665 acc_train: 0.1667 loss_val: -80.2836 acc_val: 0.1880 time: 1.5153s\n",
      "Epoch: 0014 loss_train: -87.9196 acc_train: 0.1667 loss_val: -80.6853 acc_val: 0.1880 time: 1.7560s\n",
      "Epoch: 0015 loss_train: -88.1520 acc_train: 0.1667 loss_val: -81.0533 acc_val: 0.1880 time: 1.7933s\n",
      "Epoch: 0016 loss_train: -88.3668 acc_train: 0.1667 loss_val: -81.3907 acc_val: 0.1880 time: 1.6432s\n",
      "Epoch: 0017 loss_train: -88.5675 acc_train: 0.1667 loss_val: -81.7021 acc_val: 0.1880 time: 1.6018s\n",
      "Epoch: 0018 loss_train: -88.7577 acc_train: 0.1667 loss_val: -81.9926 acc_val: 0.1880 time: 1.6809s\n",
      "Epoch: 0019 loss_train: -88.9406 acc_train: 0.1667 loss_val: -82.2675 acc_val: 0.1880 time: 1.7116s\n",
      "Epoch: 0020 loss_train: -89.1191 acc_train: 0.1667 loss_val: -82.5316 acc_val: 0.1880 time: 1.7378s\n",
      "Epoch: 0021 loss_train: -89.2951 acc_train: 0.1667 loss_val: -82.7885 acc_val: 0.1880 time: 1.5464s\n",
      "Epoch: 0022 loss_train: -89.4696 acc_train: 0.1667 loss_val: -83.0408 acc_val: 0.1880 time: 1.5228s\n",
      "Epoch: 0023 loss_train: -89.6427 acc_train: 0.1667 loss_val: -83.2897 acc_val: 0.1880 time: 1.4871s\n",
      "Epoch: 0024 loss_train: -89.8135 acc_train: 0.1667 loss_val: -83.5349 acc_val: 0.1880 time: 1.5794s\n",
      "Epoch: 0025 loss_train: -89.9803 acc_train: 0.1667 loss_val: -83.7752 acc_val: 0.1880 time: 1.8747s\n",
      "Epoch: 0026 loss_train: -90.1406 acc_train: 0.1667 loss_val: -84.0081 acc_val: 0.1880 time: 1.5450s\n",
      "Epoch: 0027 loss_train: -90.2918 acc_train: 0.1667 loss_val: -84.2310 acc_val: 0.1880 time: 1.8112s\n",
      "Epoch: 0028 loss_train: -90.4314 acc_train: 0.1667 loss_val: -84.4408 acc_val: 0.1880 time: 2.3747s\n",
      "Epoch: 0029 loss_train: -90.5573 acc_train: 0.1667 loss_val: -84.6350 acc_val: 0.1880 time: 1.7747s\n",
      "Epoch: 0030 loss_train: -90.6681 acc_train: 0.1667 loss_val: -84.8114 acc_val: 0.1880 time: 1.6111s\n",
      "Epoch: 0031 loss_train: -90.7633 acc_train: 0.1667 loss_val: -84.9691 acc_val: 0.1880 time: 1.9476s\n",
      "Epoch: 0032 loss_train: -90.8434 acc_train: 0.1667 loss_val: -85.1078 acc_val: 0.1880 time: 2.0238s\n",
      "Epoch: 0033 loss_train: -90.9094 acc_train: 0.1667 loss_val: -85.2281 acc_val: 0.1880 time: 2.0495s\n",
      "Epoch: 0034 loss_train: -90.9632 acc_train: 0.1667 loss_val: -85.3313 acc_val: 0.1880 time: 1.5991s\n",
      "Epoch: 0035 loss_train: -91.0065 acc_train: 0.1667 loss_val: -85.4189 acc_val: 0.1880 time: 1.5006s\n",
      "Epoch: 0036 loss_train: -91.0413 acc_train: 0.1667 loss_val: -85.4927 acc_val: 0.1880 time: 1.4930s\n",
      "Epoch: 0037 loss_train: -91.0694 acc_train: 0.1667 loss_val: -85.5547 acc_val: 0.1880 time: 1.4839s\n",
      "Epoch: 0038 loss_train: -91.0922 acc_train: 0.1667 loss_val: -85.6063 acc_val: 0.1880 time: 1.4594s\n",
      "Epoch: 0039 loss_train: -91.1111 acc_train: 0.1667 loss_val: -85.6492 acc_val: 0.1880 time: 1.5343s\n",
      "Epoch: 0040 loss_train: -91.1272 acc_train: 0.1667 loss_val: -85.6847 acc_val: 0.1880 time: 1.5241s\n",
      "Epoch: 0041 loss_train: -91.1413 acc_train: 0.1667 loss_val: -85.7138 acc_val: 0.1880 time: 1.4390s\n",
      "Epoch: 0042 loss_train: -91.1540 acc_train: 0.1667 loss_val: -85.7374 acc_val: 0.1880 time: 1.4607s\n",
      "Epoch: 0043 loss_train: -91.1658 acc_train: 0.1667 loss_val: -85.7562 acc_val: 0.1880 time: 1.5223s\n",
      "Epoch: 0044 loss_train: -91.1770 acc_train: 0.1667 loss_val: -85.7709 acc_val: 0.1880 time: 1.4963s\n",
      "Epoch: 0045 loss_train: -91.1880 acc_train: 0.1667 loss_val: -85.7818 acc_val: 0.1880 time: 1.4556s\n",
      "Epoch: 0046 loss_train: -91.1989 acc_train: 0.1667 loss_val: -85.7893 acc_val: 0.1880 time: 1.6755s\n",
      "Epoch: 0047 loss_train: -91.2098 acc_train: 0.1667 loss_val: -85.7936 acc_val: 0.1880 time: 1.7848s\n",
      "Epoch: 0048 loss_train: -91.2207 acc_train: 0.1667 loss_val: -85.7950 acc_val: 0.1880 time: 2.0682s\n",
      "Epoch: 0049 loss_train: -91.2318 acc_train: 0.1667 loss_val: -85.7935 acc_val: 0.1880 time: 1.6865s\n",
      "Epoch: 0050 loss_train: -91.2430 acc_train: 0.1667 loss_val: -85.7893 acc_val: 0.1880 time: 2.0212s\n",
      "Epoch: 0051 loss_train: -91.2541 acc_train: 0.1667 loss_val: -85.7823 acc_val: 0.1880 time: 1.6625s\n",
      "Epoch: 0052 loss_train: -91.2652 acc_train: 0.1667 loss_val: -85.7727 acc_val: 0.1880 time: 1.5416s\n",
      "Epoch: 0053 loss_train: -91.2761 acc_train: 0.1667 loss_val: -85.7605 acc_val: 0.1880 time: 1.5208s\n",
      "Epoch: 0054 loss_train: -91.2866 acc_train: 0.1667 loss_val: -85.7458 acc_val: 0.1880 time: 1.8521s\n",
      "Epoch: 0055 loss_train: -91.2966 acc_train: 0.1667 loss_val: -85.7288 acc_val: 0.1880 time: 1.7930s\n",
      "Epoch: 0056 loss_train: -91.3059 acc_train: 0.1667 loss_val: -85.7097 acc_val: 0.1880 time: 1.4901s\n",
      "Epoch: 0057 loss_train: -91.3144 acc_train: 0.1667 loss_val: -85.6888 acc_val: 0.1880 time: 1.6594s\n",
      "Epoch: 0058 loss_train: -91.3220 acc_train: 0.1667 loss_val: -85.6665 acc_val: 0.1880 time: 2.0394s\n",
      "Epoch: 0059 loss_train: -91.3285 acc_train: 0.1667 loss_val: -85.6434 acc_val: 0.1880 time: 2.3533s\n",
      "Epoch: 0060 loss_train: -91.3340 acc_train: 0.1667 loss_val: -85.6199 acc_val: 0.1880 time: 1.7247s\n",
      "Epoch: 0061 loss_train: -91.3383 acc_train: 0.1667 loss_val: -85.5970 acc_val: 0.1880 time: 1.6143s\n",
      "Epoch: 0062 loss_train: -91.3418 acc_train: 0.1667 loss_val: -85.5751 acc_val: 0.1880 time: 2.4525s\n",
      "Epoch: 0063 loss_train: -91.3443 acc_train: 0.1667 loss_val: -85.5551 acc_val: 0.1880 time: 1.6063s\n",
      "Epoch: 0064 loss_train: -91.3462 acc_train: 0.1667 loss_val: -85.5376 acc_val: 0.1880 time: 1.5741s\n",
      "Epoch: 0065 loss_train: -91.3477 acc_train: 0.1667 loss_val: -85.5232 acc_val: 0.1880 time: 2.0999s\n",
      "Epoch: 0066 loss_train: -91.3489 acc_train: 0.1667 loss_val: -85.5124 acc_val: 0.1880 time: 1.5778s\n",
      "Epoch: 0067 loss_train: -91.3500 acc_train: 0.1667 loss_val: -85.5053 acc_val: 0.1880 time: 1.4986s\n",
      "Epoch: 0068 loss_train: -91.3513 acc_train: 0.1667 loss_val: -85.5021 acc_val: 0.1880 time: 1.5795s\n",
      "Epoch: 0069 loss_train: -91.3528 acc_train: 0.1667 loss_val: -85.5026 acc_val: 0.1880 time: 1.6361s\n",
      "Epoch: 0070 loss_train: -91.3546 acc_train: 0.1667 loss_val: -85.5067 acc_val: 0.1880 time: 1.6587s\n",
      "Epoch: 0071 loss_train: -91.3566 acc_train: 0.1667 loss_val: -85.5139 acc_val: 0.1880 time: 1.6466s\n",
      "Epoch: 0072 loss_train: -91.3589 acc_train: 0.1667 loss_val: -85.5237 acc_val: 0.1880 time: 1.7541s\n",
      "Epoch: 0073 loss_train: -91.3612 acc_train: 0.1667 loss_val: -85.5355 acc_val: 0.1880 time: 1.7867s\n",
      "Epoch: 0074 loss_train: -91.3636 acc_train: 0.1667 loss_val: -85.5487 acc_val: 0.1880 time: 1.6089s\n",
      "Epoch: 0075 loss_train: -91.3659 acc_train: 0.1667 loss_val: -85.5628 acc_val: 0.1880 time: 1.6031s\n",
      "Epoch: 0076 loss_train: -91.3681 acc_train: 0.1667 loss_val: -85.5772 acc_val: 0.1880 time: 2.0199s\n",
      "Epoch: 0077 loss_train: -91.3701 acc_train: 0.1667 loss_val: -85.5915 acc_val: 0.1880 time: 1.7532s\n",
      "Epoch: 0078 loss_train: -91.3719 acc_train: 0.1667 loss_val: -85.6052 acc_val: 0.1880 time: 1.5995s\n",
      "Epoch: 0079 loss_train: -91.3735 acc_train: 0.1667 loss_val: -85.6180 acc_val: 0.1880 time: 1.4205s\n",
      "Epoch: 0080 loss_train: -91.3748 acc_train: 0.1667 loss_val: -85.6297 acc_val: 0.1880 time: 1.6671s\n",
      "Epoch: 0081 loss_train: -91.3760 acc_train: 0.1667 loss_val: -85.6401 acc_val: 0.1880 time: 2.0019s\n",
      "Epoch: 0082 loss_train: -91.3770 acc_train: 0.1667 loss_val: -85.6491 acc_val: 0.1880 time: 1.9390s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0083 loss_train: -91.3780 acc_train: 0.1667 loss_val: -85.6566 acc_val: 0.1880 time: 1.6902s\n",
      "Epoch: 0084 loss_train: -91.3788 acc_train: 0.1667 loss_val: -85.6626 acc_val: 0.1880 time: 2.0107s\n",
      "Epoch: 0085 loss_train: -91.3797 acc_train: 0.1667 loss_val: -85.6671 acc_val: 0.1880 time: 1.6891s\n",
      "Epoch: 0086 loss_train: -91.3805 acc_train: 0.1667 loss_val: -85.6701 acc_val: 0.1880 time: 1.7663s\n",
      "Epoch: 0087 loss_train: -91.3813 acc_train: 0.1667 loss_val: -85.6719 acc_val: 0.1880 time: 1.6225s\n",
      "Epoch: 0088 loss_train: -91.3822 acc_train: 0.1667 loss_val: -85.6723 acc_val: 0.1880 time: 1.5805s\n",
      "Epoch: 0089 loss_train: -91.3831 acc_train: 0.1667 loss_val: -85.6717 acc_val: 0.1880 time: 1.4664s\n",
      "Epoch: 0090 loss_train: -91.3841 acc_train: 0.1667 loss_val: -85.6700 acc_val: 0.1880 time: 1.4446s\n",
      "Epoch: 0091 loss_train: -91.3850 acc_train: 0.1667 loss_val: -85.6675 acc_val: 0.1880 time: 1.4596s\n",
      "Epoch: 0092 loss_train: -91.3860 acc_train: 0.1667 loss_val: -85.6643 acc_val: 0.1880 time: 1.5390s\n",
      "Epoch: 0093 loss_train: -91.3869 acc_train: 0.1667 loss_val: -85.6605 acc_val: 0.1880 time: 1.4544s\n",
      "Epoch: 0094 loss_train: -91.3878 acc_train: 0.1667 loss_val: -85.6564 acc_val: 0.1880 time: 1.4943s\n",
      "Epoch: 0095 loss_train: -91.3887 acc_train: 0.1667 loss_val: -85.6520 acc_val: 0.1880 time: 1.9180s\n",
      "Epoch: 0096 loss_train: -91.3895 acc_train: 0.1667 loss_val: -85.6477 acc_val: 0.1880 time: 1.7410s\n",
      "Epoch: 0097 loss_train: -91.3903 acc_train: 0.1667 loss_val: -85.6435 acc_val: 0.1880 time: 1.6610s\n",
      "Epoch: 0098 loss_train: -91.3910 acc_train: 0.1667 loss_val: -85.6396 acc_val: 0.1880 time: 1.6502s\n",
      "Epoch: 0099 loss_train: -91.3917 acc_train: 0.1667 loss_val: -85.6360 acc_val: 0.1880 time: 1.6055s\n",
      "Epoch: 0100 loss_train: -91.3924 acc_train: 0.1667 loss_val: -85.6330 acc_val: 0.1880 time: 1.7588s\n",
      "Epoch: 0101 loss_train: -91.3930 acc_train: 0.1667 loss_val: -85.6306 acc_val: 0.1880 time: 1.9647s\n",
      "Epoch: 0102 loss_train: -91.3936 acc_train: 0.1667 loss_val: -85.6289 acc_val: 0.1880 time: 1.5739s\n",
      "Epoch: 0103 loss_train: -91.3942 acc_train: 0.1667 loss_val: -85.6278 acc_val: 0.1880 time: 1.5895s\n",
      "Epoch: 0104 loss_train: -91.3948 acc_train: 0.1667 loss_val: -85.6274 acc_val: 0.1880 time: 1.4081s\n",
      "Epoch: 0105 loss_train: -91.3954 acc_train: 0.1667 loss_val: -85.6277 acc_val: 0.1880 time: 1.4275s\n",
      "Epoch: 0106 loss_train: -91.3960 acc_train: 0.1667 loss_val: -85.6286 acc_val: 0.1880 time: 1.7693s\n",
      "Epoch: 0107 loss_train: -91.3966 acc_train: 0.1667 loss_val: -85.6300 acc_val: 0.1880 time: 2.3195s\n",
      "Epoch: 0108 loss_train: -91.3971 acc_train: 0.1667 loss_val: -85.6319 acc_val: 0.1880 time: 1.6623s\n",
      "Epoch: 0109 loss_train: -91.3977 acc_train: 0.1667 loss_val: -85.6341 acc_val: 0.1880 time: 1.6143s\n",
      "Epoch: 0110 loss_train: -91.3982 acc_train: 0.1667 loss_val: -85.6366 acc_val: 0.1880 time: 1.5580s\n",
      "Epoch: 0111 loss_train: -91.3988 acc_train: 0.1667 loss_val: -85.6393 acc_val: 0.1880 time: 1.6218s\n",
      "Epoch: 0112 loss_train: -91.3993 acc_train: 0.1667 loss_val: -85.6420 acc_val: 0.1880 time: 1.9107s\n",
      "Epoch: 0113 loss_train: -91.3998 acc_train: 0.1667 loss_val: -85.6447 acc_val: 0.1880 time: 1.5276s\n"
     ]
    }
   ],
   "source": [
    "training_log = []\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "temp_val_loss = 999999\n",
    "temp_test_loss = 0\n",
    "temp_test_acc = 0\n",
    "PATH = \"save/model_param/{}{}.pt\".format(model_name, dataset_name)\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    epo, trainloss, trainacc, valloss, valacc, testloss, testacc, epotime = train(epoch)\n",
    "    training_log.append([epo, trainloss, trainacc, valloss, valacc, testloss, testacc, epotime])\n",
    "    \n",
    "    if valloss <= temp_val_loss:\n",
    "        temp_val_loss = valloss\n",
    "        temp_test_loss = testloss\n",
    "        temp_test_acc = testacc\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "print(\"Best result:\",\n",
    "          \"val_loss=\",temp_val_loss,\n",
    "            \"test_loss=\",temp_test_loss,\n",
    "             \"test_acc=\",temp_test_acc)\n",
    "bestmodel = torch.load(PATH)\n",
    "if model_name == 'AGNN':\n",
    "    print(\"the weight is: \", torch.softmax(bestmodel['gc1.linear_weight'].data,dim=0))\n",
    "# output, fp1, fp2 = model(feats, adj)\n",
    "# test_LDA = LDA_loss(fp1[idx_train], Y_onehot[idx_train], nb_each_class_inv_mat_test, norm_or_not = False)\n",
    "# print(\"test_LDA: test_LDA\")\n",
    "\n",
    "# # Testing\n",
    "# test()\n",
    "\n",
    "# # save training log\n",
    "# # expname = input('input experiment name: ')\n",
    "# expname = dataset_name + '_' + model_name\n",
    "# log_pk = open('./save/trainlog_'+expname+'.pkl','wb')\n",
    "# pkl.dump(np.array(training_log),log_pk)\n",
    "# log_pk.close()\n",
    "# print(\"finish save log\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
